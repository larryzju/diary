#+TITLE: CSAPP 摘要
#+AUTHOR: Zhao WenBin
#+STATUS: 55/1078
#+OPTIONS: toc:2 H:4
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="style.css">

* A Tour of Computer System

Computer system consists of
1. hardward
2. system software
3. application programs

The underlying concepts are the same event for different implements.

常见的问题如数字格式、C 程序优化、链接、内存泄露、并行程序开发等。都将在后续展开。

** Infomation is Bits + Context

计算机中所有的信息都以一系列的位来表示。例如，源代码文件，以字节（8位）为单位保存在文件中。其编码由 ASCII 定义。

不同的位表示不同的意思的本质在于上下文（Context）。


#+BEGIN_aside
C 语言于 1969-1973 年在贝尔实验室被创造。由 ANSI C 和 ISO 标准化，著名书籍是 K&R。

C program language has below features
1. portable and associated with UNIX tightly
2. small and simple
3. paratical purpose
#+END_aside

** Programs Are Translated by Other Programs into different form

程序以人类可读的程序语言写成（如 C 语言或汇编语言），需要将之翻译为机器可以识别的机器语言（二进制的可执行文件）。

通常的 C 源码需要经过 4 个步骤生成最终可执行文件，如下

| STEPS             | DESCRIPTION                   |
|-------------------+-------------------------------|
| preprocessor(cpp) | 用于展开头文件(.i)            |
| compiler(cc1)     | 翻译成汇编文件(.s)            |
| assembler(as)     | 汇编生成 relocatable 文件(.o) |
| linker(ld)        | 生成可执行文件                |

#+BEGIN_aside
GNU 项目由 Richard Stallman 于 1984 年发起，提供了诸多工具，如 emacs, gcc, gdb, as, linker 等 。linux 亦是基于 GNU 提供的工具构建的。
#+END_aside


** It Pays to Understand How Compilation System Work

编译器作了太多的事情，为此我们需要了解其细节，以便在以下方面进行优化：

1. 优化性能：只有知道了编译器原理才能写出更好的高级语言。常见的问题如
   - switch 与 if-else 的性能对比
   - 函数调用开销有多大
   - while 和 for 的开销
   - pointer 和 array 的开销
   - 解引用与直接访问本地变量的开销
   - 算术顺序对性能的影响
   - I32 与 x86-64 的指令系统
   - 内存管理
2. Understand link-time error，例如
   - 找不到引用
   - static 与 global 变量
   - global 变量重名
   - 静态库与动态库
   - 库的顺序影响正确性
   - 运行时发生的错误
3. Avoiding security holes：
   - 限制从未知源获取数据
   - 栈的存储方式可能造成信息泄漏

** Processors Read and Interrupt Instructions Stored in Memory

正常执行程序时，通过 shell 调用可执行程序。实际在底层复杂无比，归纳起来过程如下：

1. shell 交互输入参数到内存
2. 加载可执行文件内容到内存（DMA 技术）
3. 执行
4. 退出 

其中涉及到计算机的硬件组成。如下图所示：

#+CAPTION: Hardware organization
[[./img/hardware-org.png]]

硬件中包含几个核心部件，如 CPU，内存，IO 设备，总线等。

- 总线 :: 用于沟通各个组件，以 word 为单位（4字节或8字节）进行传送
- IO 设备 :: 包括输入、输出、存储和网络，通过 controller/adapter[fn:1] 接到 IO 总线上
- 主存 :: DRAM (Dynamic Random Access Memory)，通过唯一的连续地址来访问
- 处理器 :: CPU 核心（如 PC，寄存器，ALU单元），根据指令集[fn:2]进行取指、择指、执行、下一条的循环过程，常见操作如 load,store,operate,jump 等

** Caches Matters

机器指令中的很大一部分工作都是在外部存储、内存、寄存器和外设之间移动数据。

而存储的容量与读写的效率成反比，而且差距还在增大。所以需要考虑如何优化
拷贝的效率。一个关键的问题称为 processor-memory gap (处理器性能与内存
读取的鸿沟)。通过添加 cache memory 来缓存近线数据来弥补数据的速度与容
量的差距性。

** Storage Devices From a Hierarchy

#+CAPTION: Memory Hierarchy
[[file:img/memory-hierarchy.png]]

主流解决方案采用三级缓存，CPU 缓存使用 SRAM(static random access memory) 技术。

其中寄存器视为第 0 级缓存，自上向下速度越来越慢，而存储空间越来越大。

** The Operating System Manages the Hardware

操作系统作为中间层，隔离了应用软件与硬件，其作用有两个：

1. 保护硬件的正常运行
2. 提供对硬件复杂性的封装，提供一致的、高效的接口

操作系统对硬件作出抽象，其中主要包括

- files
- virtual memory
- processor

#+BEGIN_aside
- Unix :: 诞生于 Multics 项目之后，由贝尔实验室 Ken Tompson 和 Dennis Ritchie 等在 DEC PDP-7 机器上实现，于 1973 年用 C 重写，并于 1974 年对外发布。
  + BSD :: 伯克利版本 UNIX，加入了 Internet 支持和虚拟内存支持
  + System V. :: 贝尔实验室后续版本
  + Solaris :: Sun 公司基于二者开发的版本
- Posix :: IEEE 牵头制定的 UNIX 标准，包括系统调用的 C 接口，shell, utilities，thread 及 network programming。由 Richard Stallman 命名
#+END_aside

*** Processes

进程是对正在运行的程序的一个抽象描述。每个进程都有自己独立的计算、内存及 IO 资源。

操作系统通过上下文切换来时分复用，达到多个进程同时运行（concurrently）的效果。多核环境下，可以有真正的并行（parallelism）

上下文切换需要保存 PC 指针、Register file、Content of main memory 信息，涉及到操作系统与硬件的配合

*** Threads

在一个进程中的上下文中可以有多个线程，线程共享代码和全局数据，因此更容易共享数据，效率较高

*** Virtual Memory

在每个进程看来，自己独占了整个内存，称为 virtual address space。实际上
由操作系统在上下文切换时，通过硬件的转换机制伪造出相应的内存视图。可以
看作用硬盘来保存进程的虚拟内存，而使用 main memory 作为缓存提高效率。

进程的内存分为多个区域，存储代码和数据，以及某些公共区域，如下图所示

#+CAPTION: Virtual Memory
[[file:img/virutal-memory.png]]

其中自下往上依次为：
- code 在放在低地址的固定区域
- C 中的全局变量存储
- 堆区域，在运行时通过 malloc 和 free 来动态扩展
- shared libraries，用于动态链接
- stack，保存函数调用时参数和返回传递，向下增长
- kernel virtual memory 位于高地址，程序不可用

*** Files

文件是对 I/O 设备的一种抽象。其本质是一堆字节，并提供了读写操作。文件
的作用是统一了 I/O 设备的视图，并提供不同设备、不同平台间的移植性。

Unix 中一切皆文件，因此也称作 Unix I/O

#+BEGIN_aside
- Linux Project :: Linus Torvalds 于 1991 年作为业余项目实现的一个类 UNIX 操作系统，受 Minix 启发，并兼容 Posix 标准。与 GNU 项目关系密切
#+END_aside

** System Communicate with Other System Using Network

计算机系统并不只是以单机运行的。越来越多的价值从互联之中发掘。计算机通过网络与其它计算机交换数据，网络也是一种 I/O 设备

** Important Themes
*** Concurrency and Parallelism

There are two simliar concepts about multiple execution implement ways
- Concurrency :: To do more at the same time
- Parallelism :: To do more and also fast at the same time

There're 3 levels of parallelism abstract
1. Thread Level
2. Instruction Level
3. SIMD Level

**** Thread Parallelism

Each Process can have multiple control flows. Operation System
switch between threads (by backup and restore its context) rapidly
in time shared manner to simulate parallel executions.

The origin aims of multiple threads is for the benefits of :
- Multiple users
- Single user with multiple tasks

In the early days, the *uniprocessor system*'s multiple threads are
just simulation for parallelism. The later *multiprocessor system* can
run threads in real parallel manners.  

There are 2 technologies in multiple processors system:
- multiple cores :: each core has its own L1/L2 cache and resides in the same chip
- hyperthreading :: each CPU has multiple copies of register and
                    hardwares, can execute multiple tasks at same time

Parallelism can be benefit for improve performances in aspects:
1. real multiple tasks
2. need new methods to program that can dig out the advantages of multiprocessor

**** Instruction Parallelism

The most famous method is *pipeline* which execute instruction's setps
in parrel and improve the performance faster than execute instruction
for the whole cycles.

The technology to execution rates if faster than one instruction per
cycle is called *superscalar*

**** SIMD Parallelism

- SIMD is short for Single Instruction Multiple Data.
- Mostly used to speed up image, sound, video procses.
- Need language (data type) and compiler supports.

*** The Importance of Abstraction of Computer System

*Abstraction* is one of the most important concept in Computer Science.

The import abstractions are:
- API :: prototype which user doesn't need to delve into details
- Program Language :: Abstraction concept like class, function
- ISA (Instruction Set Architecture) :: Abstraction of sequential
     execution model for hardware implements (Different hardware can
     have different implement but share the machine code)
- File :: Abstraction of I/O
- Virtual Memory :: Abstraction Linear memory for program
- Process :: Abstraction of a running program
- Virtual Machine :: Abstraction of computer hardware

* Representing and Manipulating Informations

** Overview

Computer information is represented in binary mode. The reason to
express information with 2-vlaue bits is because its readily be
represented, stored and transmitted.

We group bits and apply rules to interprete them for given meaning of
bit pattern. That is called Data Type.

The reasons why we programmers should dive deep into the
representation of data are listed below:
1. write correct and portable program for different platforms
2. improve performance
3. security cause
4. understand machine-level program

Different types of number has three representations
1. unsigned integer
2. signed integer
3. floating-point

For integer representations, it can be encoded for a comparatively
small range of values, but do so precisely. While floating-point
representations can encode a wide range of values, but only
approximately.

Here's an example to show why we should be careful for the
representation of data (floating-point arithmetic doesn't conform the
association rule):

#+BEGIN_SRC c
  (3.14 + 1e20) - 1e20            /* 0.0 */
  3.14 + (1e20 - 1e20)            /* 3.14 */
#+END_SRC


*** C standard                                                            :c:

For GNU c compiler
1. =--ansi= or =--std=c89=: ANSI C(1989)/ISO C99(1990)
2. =--std=gnu89=: GNU extends for ANSI C
3. =--std=c99=: ISO C99(1999)
4. =--std=gnu99=: GNU extends for ISO C99

** Information Storage

We use *bytes* as the unit of memory which is consisted by 8 bits.

The virtual memory that program visits can be regarded as an array of
bytes. Each bytes is indexed by *address*. Underlying the virtual
memroy, it's made up of RAM, disk storage, special hardware, OS and
provides the program with what appears to be a monolithic bytes array.

~Pointer = Address + DataType~ 

Actually, C compiler maintains pointer's type information, the
machine-level program it generates has no information about data
types.

Pointer are the central feature of C:
1. can be used to refer elements of data structure
2. combine value and type (provide flexible in program language)

*** Hexadecimal Notation

Because of the representation in binary notation is too tedious. It's
convinent to use hexadecimal notation to represent integer (such as
=0xFA1D37BB=)

It's necessary to know how to convert among binary, decimal and
hexadecimal:
- hex to bin :: expand each hexadecimal to 4 bits binary
- bin to hex :: split into groups of 4 bits (make the leftmost group
                be the one with fewer than 4bits and padding with
                leading zeros)
- hex to dec :: multiplication methods
- dec to hex :: division methods

Some useful conversion or notation list below:
| 2^7 | 128 |
| 2^8 | 256 |
| 2^10 | 1024 |
| 2^16 | 65536 |

And $2^n$ in binary notation is a string starts with 1 followed by n
zero.

*** Words

Bytes group to word. It's the nominal size of integer and pointer
data.

The word is used to express virtual address. For 32bit platform, the
memory address is in range from 0 to $2^32-1$ (4G bytes).

The 64bit platform extends the max available memory address to
$2^64-1$.

*** Data Sizes

The typeical data sizes (in C language) shows in the table below:
| data type | 32 bit | 64 bit | note                                                                 |
|-----------+--------+--------+----------------------------------------------------------------------|
| char      |      1 |      1 |                                                                      |
| short int |      2 |      2 |                                                                      |
| int       |      4 |      4 |                                                                      |
| long int  |      4 |      8 | the main different between 32bit and 64bit platforms                 |
| long long |      8 |      8 | defined in ISO C99 standard. supported by compiler in 32bit platform |
| char*     |      4 |      8 | word size of platform                                                |
| float     |      4 |      4 |                                                                      |
| double    |      8 |      8 |                                                                      |

The real data sizes depends on both the machine and the compiler.

It's very important to write portable code which is insentitive to the
exact sizes of different data types.

For example, in history, =int= can be used to store a pointer in 32bit
platform. But it's a fault error in 64bit platform.

** Integer Representations

** Integer Arithmetic

** Floating Point 

* TODO [2/12] Machine-Level Representation of Programs

A compiler generates machine code through a series of stage, based on

- the rules of the programming language, 
- the instruction set of the target machine, 
- the conventions followed by the operating system.

Stages of compile with GCC

1. generate assembly code
2. assembler: generate machine code
3. linker: generate the executable machine code


With the compiler

1. program in a higher level
2. detect many program errors and make sure we reference and manipulate data in consistent ways
3. optimize the generated code
4. cross multiple platform and machine

Why should we learn machine code?

- being able to read and understand it is an important skill for serious programmers
- understand the optimizations capabilities and analyze the underlying inefficiencies in the code
- understand the data share in multiple threads and other machine-code level hidden by the program language
- understand how the vulnerabilities arise and how to guard against them

(与汉语识繁用简同理)


Key knowledge

- relation between C, assembly code, and machine code
- representation and manipulation of data
- implementation of control: if, while, switch
- implementation of procedures, including how the program maintains a run-time stack, as well as storage for local variables
- implementation of data structures: structure , unions, arrays
- the problems of out-of-bounds memory reference
- buffer overflow attacks
- GDB debugger

** DONE A Historical Perspective

x86 CPU series

- 8086 :: 1978, 29K transistor, 16-bit microprocessors. 8088 is a variant of the 8086 with an 8-bit external bus. Used
          as the CPU of the MS-DOS operating system. 1980, Intel introduced 8087 floating-point coprocessor
- 80286 :: 1982, 134K transistor. Formed the basis of the IBM PC-AT PC
- i386 :: 1985, 275K, 32-bits. Added the flat addressing model, could fully support a Unix operating system
- i486 :: 1989: 1.2M transistor, integrated the floating-point unit
- Pentium :: 1993, 3.1M transistor
- PentiumPro :: 1995, 5.5M transistor, add P6 microarchitecture
- Pentium/MMX :: 1997, 4.5M transistor, add intructions to manipulate vector of integers
- Pentium II :: 1997, 7M transistor
- Pentium III :: 1999, 8.2M transistor, Introduce SSE (manipulate vectors of integer or floating-point data)
- Pentium 4 :: 2000, 42M transistor, extended SSE to SSE2
- Pentium 4E :: 2004, 125M transistor, add hyperthreading and EM64T (x84-64)
- Core 2 :: 2006, 291M transistor, fist multi-core Intel microprocessor
- Core i7, Nehalem :: 2008, 781M transistor
- Core i7, Sandy Bridge :: 2011, 1.17G transistor, introduced AVX
- Core i7, Haswell :: 2013, 1.4G transistor, extended AVX to AVX2


Terms

- IA32 :: Intel Architecture 32-bit
- Intel64 :: the 64-bit extension to IA32, refer to as x86-64


#+BEGIN_ASIDE
Moore's Law

The number of transistors doubles about every 26 months.
#+END_ASIDE


** DONE Program Encodings

#+BEGIN_SRC sh
  > gcc -Og -o p p1.c p2.c
#+END_SRC

=-Og= instructs the compiler to apply a level of optimization that yields machine code that follows the overall structure
of the origin C code.

Sequence of programs to turn the source code into executable code
1. preprocessor expand macros
2. compiler generate assembly-code version (.s)
3. assembler converts the assembly code into binary object-code (.o)
4. linker merges object-code files along with code implementing library function, generate the executable code file

*** Machine-Level Code

1. ISA (instruction set architecture) define the format and behavior of a machine-level program
2. virtual address provides a memory model 

Assembly-code representation is very close to machine code but in a more readable textual format.

C programmer cannot see some processor state, such as
- program counter (PC, %rip in x6464)
- 16 named register file
- condition code registers: used to implement conditional changes in the control or data flow
- set of registers hold integer or floating-point value

Machine-level code doesn't have data types (integer, unsigned integer, pointer, string, array or structure) but a
contiguous collections of bytes.

64-bit CPU can address up to $2^48$ (64 TB) memory

*** Code Examples 

Command lines 
- Generate assembly code with =gcc -Og -S mstore.c=
- Compile and assemble the code with =gcc -Og -c mstore.c=
- Inspect the object file (disassemblers): =objdump -d mstroe.o=
- Generate an executable program =gcc -Og -o prog main.c mstore.c=

Note:

- x86-64 instructions can range in length from 1 to 15 bytes
- Linker shifts the location of code to a different range of addresses
- Linker insert the real function address
- The generated =mstore.s= have directive beginning with "." to guide the assembler and linker

#+BEGIN_ASIDE
ATT vs Intel assembly-code format

GNU tools use AT&T format, use =gcc -Og -S -masm=intel mstore.c= to generate Intel format.

The differences are

- Intel code omits the size designation suffixes (for example using =push= other than =pushq=)
- Intel code omits the '%' character in front of register names, using =rbx= instead of =%rbx=
- Intel code has a different way to describe location in memory, using =QWORD PTR [rbx]= rather than =(%rbx)=
- Instructions with multiple operands list them in the reverse order

#+END_ASIDE

** TODO Data Formats

** TODO Accessing Information

** TODO Arithmetic and Logical Operations

** TODO Control

** TODO Procedures

** TODO Array Allocation and Access

** TODO Heterogeneous Data Structures

** TODO Combining Control and Data in Machine-Level Programs

** TODO Floating-Point Code

** TODO Summary

* [1/7] TODO 处理器体系结构

** DONE 概述

处理器操作如基本指令控制，指令(instruction) 被编码成一个或多个字节。
不同的处理器有自己的一套指令集和编码格式，称为 ISA （instruction-set architecture）

ISA 提供了一个抽象层，编译器开发者需要与之打交道。
IA32 是一个比较大众的 ISA，被多家厂商使用与支持。

实际的硬件实现又与 ISA 模型有很大不同。处理器可能使用流水线技术，并行地处理多条指令，以提高性能。
但对于使用者来说，仍然感觉到指令是在被顺序执行的。

*** 为什么要关心处理器的设计

- 有趣，处理器包含了很多工程上的巧妙设计，反映如何用简单的方法来解决复杂的问题
- 有助于理解计算机体系
- 嵌入式系统
- 假如你刚好从事这个行业

*** TODO Y86

本书为了介绍 I32 （也称为 X86）而简化的一种处理器结构。
并讲解从 sequence processor 如何进化成为 pipelined processor



** [2/6] TODO The Y86 Instruction Set Architecture

*** DONE Programmer-Visible State

一些可以被开发人员（汇编开发者或编译器开发者）访问的对象，包括

- program register :: %eax, %ecx, %edx, %ebx, %esi, %edi, %esp, %ebp (其中 %esp 为栈指针)
- condition code :: 逻辑运算结果，包括三个位 ZF, SF, OF
- PC :: 指向下一条指令
- Program Status Word :: 记录异常信息
- Memory :: a large array of bytes

**** TODO figure 4.1

*** DONE Y86 Instructions

精简后的指令有

- halt :: 停止
- nop :: 空
- mov :: 有四种（针对内存、寄存器、立即数），包括 rrmovl, irmovl, rmmovl, mrmovl
- op :: 四种计算：addl, subl, andl, xorl，只能针对寄存器
- jump :: 七种，根据 condition code 来跳转，包括 jmp, jle, jl, je, jne, jge, jg
- condition move :: 六种，包括 cmovle, cmovl, cmove, cmovne, cmovge, cmovg
- call :: 压栈，并跳转到新函数
- ret :: 从函数中返回
- pushl :: 压栈
- popl :: 弹栈

**** TODO figure 4.2


*** TODO Instruction Encoding

*** TODO Y86 Exception

*** TODO Y86 Programs

*** TODO Some Y86 Instruction Details

** TODO Logic Design and the Hardware Control Language HCL

** TODO Sequential Y86 Implementations

** TODO General Principles of Pipelining

** TODO Pipelined Y86 Implementations

** TODO Summary

* [0/0] The Memory Hierarchy

Simple mode of a computer
1. CPU executes instructions
2. memory system is a linear array of bytes
3. CPU can access each memory location in a constant amount of time


In practice, a _memory system_ is a hieararchy of storage devices with
- different capacities
- costs
- access times


Here's the list of memory devices with speed from fast to slow

| device       | CPU cycle to read |
|--------------+-------------------|
| CPU register |                 0 |
| Cache memory |              1-30 |
| Main memory  |            50-200 |
| Disk         |             10^10 |


#+begin_quote
The overall effect is a large pool of memory that costs as much as the
cheap storage near the bottom of the hierarchy, but that serves data
to programs at the rate of the fast storage near the top of the hierarchy.
#+end_quote


**locality** : programs with good locality tend to access more data
items from the upper levels of the memory hierarchy than programs with
poor locality, and thus run faster.


Topics of this chapter
- Basic storage technologies
  - SRAM
  - DRAM
  - ROM
  - rotating and solid state disks
- cache memory as staging area between CPU and main memory
- analyze locality, how to improve the locality in your programs
- "memory mountain"

** TODO [0/4] Storage Techonologies
*** TODO [6/7] Random-Access Memory

Two varieties
1. static (faster and expensive, used for cache memory, both on/off CPU cihp)
2. dynamic (main memory, frame buffer of graphic display)
   
**** DONE Static RAM

- 6-transistor circuit for each cell
- bistable: only two types of voltage configurations, or *state*
- retain its value indefinitely, as long as it is kept powered

**** DONE Dynamic RAM

- store bit as charge on a capacitor with a single access transistor
- very sensitive to any disturbance (exposure to light rays will cause change)
- current leakage, OS need to refresh every bit of memory

**** DONE Conventional DRAMs

supercell
- cells are partitioned into d supercells
- each supercell has w DRAM cells
- supercells are organized as a rectangular array with r rows and c columns

memory controller communicate with DRAM chip to get/put w bits at one time
1. 8bit data wire
2. 2bit addr wire

#+CAPTION: reading the contents of a DRAM supercell
[[./img/read-dram-supercell.png]]

**** DONE Memory Modules

DRAM chips are packaged in memory modules that can be plug into motherboard.

There are two types of interface
1. DIMM (dual inline memory module) 168-pin, transfer in 64-bit chunk
2. SIMM (single inline memory module) 72-pin, transfer in 32-bit chunk

#+CAPTION: read via memory module
[[./img/memory-module.png]]

In above figure,
- 8 DRAM chips, each has 8M storage
- each supercell store 1 byte of main memory
- bytes from same position (i,j) of every chip build into a doubleword (64-bits)
- memory controller communicate with 8 DRAM chips and return 64-bit doubleword to CPU


***** Practice Problem 6.1 (?)

| Organization |    r | c | $b_r$ | $b_c$ | $max(b_r, b_c)$ |
|--------------+------+---+-------+-------+-----------------|
|         16x1 |   16 | 1 |     4 |     1 |               4 |
|         16x4 |   16 | 4 |     4 |     2 |               4 |
|        128x8 |  128 | 8 |     7 |     3 |               7 |
|        512x4 |  512 | 4 |     9 |     2 |               9 |
|       1024x4 | 1024 | 4 |    10 |     2 |              10 |

**** DONE Enhanced DRAMs

Types of new DRAMs
- Fast page mode DRAM (FPM DRAM) :: optimize read consecutive columns in same row
- Extended data out DRAM (EDO DRAM) :: optimized FPM DRAM
- Synchronous DRAM (SDRAM) :: synchronous controller signal, faster than FPM or EDO (async mode)
- Double Data-Rate Synchronous DRAM (DDR SDRAM) :: enhanced SDRAM,
  using both clock edges as control signals to double the speed. Have
  different prefetch buffer size: DDR (2bit), DDR2 (4bit), DDR3 (8bit)
- Rambus DRAM (RDRAM) :: proprietary tech like DDR SDRAM
- Video RAM (VRAM) :: used in frame buffer, like FPM DRAM, support concurrent I/O


History
- Until 1995, FPM DRAMs
- 1996-1999, EDO DRAMs
- SDRAMs 1995-2002
- By 2010, DDR3 SDRAM  (Intel Core i7 supports only DDR3 SDRAM)

**** DONE Nonvolatile Memory

Nonvolatile memory is the one can retain their information even when
power is off.

It is named ROM (read-only memory), even though some types of ROMs can
be written to as well as read.

ROMs are distinguished by the number of times they can be rewrite, and
the mechansim for reprogramming them:
- programmable ROM (PROM) :: be programmed exactly once, a sort of fuse
- erasable programmable ROM (EPROM) :: erase by shining ultraviolet
  light, can rewrite 1000 times
- eletrically erasable PROM (EEPROM) :: up to 10^5 times rewrite
- flash memory :: SSD


Firmware: program stored in ROM device
- PC BIOS
- graphics card/disk driver controller firmware to translate I/OI
  request from CPU
  
**** TODO Accessing Main Memory




*** TODO Disk Storage
*** TODO Solid State Disks
*** TODO Storage Technology Trends
** TODO Locality
** TODO The Memory Hierarchy
** TODO Cache Memories
** TODO Writing Cache-friendly Code
** TODO Putting it Together: The Imp[act of Caches on Program Performance
** TODO Summary
* [8/13] Virtual Memory

** DONE Overview

与 CPU 共享不同，CPU 共享最多因为进程数增大而变慢。内存共享可能会有其它问题
1. 某些进程得不到需要的内存
2. 内存内容被破坏

Modern systems provide an abstraction of main memory known as /virtual memory/ (VM).

Virtual memory is an elegant interaction to provide each process with a address space that is
- large
- uniform
- private 

The aspects to interact with are
- hardware exception
- hardware address translation
- main memory
- disk file (swap)
- kernel software 

Virtual memory provides 3 important capabilities:
- swap :: use main memory effciently by treating it as a cache for an
          address space stored on disk, keeping only the active areas
          in main memory and transferring data back and forth between
          disk and memory as needed
- flat address :: it simplifies memory management by providing each
                  process with a uniform address space
- isolation :: it protects the address space of each process from
               corruption by other processes

*** Why would a programmer need to understand it?

- central :: pervades all levels of computer systems: hardware
             exception, assemblers, linkers, loaders, shared objects,
             files and processes
- powerful :: =malloc=, =mmap=, share memory with other processes
- dangerous :: segmentation fault or protection fault

*** Two angles to learn virtual memory

1. how does it work
2. how it is used and managed by application

** DONE Physical and Virtual Addressing

- PA (physical address) :: is organized as an array of M contiguous
     byte-size cells.
- VA (virtual address) :: is converted to the appropriate physical
     address before being sent to main memory.
- MMU (Memory management unit) :: using a lookup table stored in main
     memory, whose contents are managed by the operating system, to
     perform the address transferring

** DONE Address Spaces

/linear address space/ is the consecutive address space that is
represented with the number of bits (32-bit or 64-bit).

Each data object, represented with several bytes, has the address
attribute. Each object can have multiple independent addresses, each
chosen from a different address space. (?)

| VA bits | Number of VA | Largest possible virtual address |
|---------+--------------+----------------------------------|
|       4 | 16           | 15                               |
|      14 | 16K          | 16K-1                            |
|      24 | 16M          | 16M-1                            |
|      46 | 64T          | 64T-1                            |
|      54 | 16P          | 16P-1                            |

** DONE VM as a Tool for Caching

Cache the contiguous addressed disk space into virtual memory in unit of fix sized block:
- /virtual pages/ (VPs) :: for virtual memory
- /physical pages/ (PPs) :: for physical memory, also referered as /page frames/

Virtual pages is partitioned into 3 disjoint subsets:
- Unallocated :: pages have not yet been allocated by VM system (has no data associated with)
- Cached :: cached in physical memory
- UnCached :: allocated pages that are not cached in physical memory (only in disk)

By the cache meachanism, system can provide larger virtual address space that the available physical address.

#+CAPTION: VM as Cache
[[./img/9.3.vp-pp-mapping.jpg]]

*** DRAM Cache Organization

**** Term

- *SRAM* cache denotes the L1, L2, and L3 cache memories between the CPU and main memory. 
- *DRAM* cache to denote the VM system's cache that caches virtual pages in main memory.

**** Compare

DRAM is important because of
1. read from disk is too slow (about 100,000 slower than a DRAM)
2. read the first byte from a disk sector is about 100,000 times slower than reading successive bytes in the sector

The bottom line is that the organization of the DRAM cache is driven entirely by the enormous cost of misses.

*** Page Tables

Determine 
- which *physical page* it is cached in if a *virtual page* is cached
- Or the data is stored on disk
  + determine where it is stored
  + select a victim page in physical memory
  + copy the virtual page from disk to DRAM, replacing the victim page

These capabilites are provided by
1. *MMU* (hardware) for address translating: read the mapping rules from *page table*
2. *page table*: OS maintain a data structure in physical memory that maps virtual pages to physical pages

#+CAPTION: Page Table
[[./img/page-table.png]]

In the figure above:
- PTE (/page table entries/): is the element type of page table, contains =valid= and =address= fields
- each page in VA space has a PTE at a *fixed offset* in the page table

| Valid | Address | Description                                                         |
|-------+---------+---------------------------------------------------------------------|
|     1 | addr    | virtual page is currently cached in physical address `addr` in DRAM |
|     0 | null    | unallocated                                                         |
|     0 | addr    | address points to the start of the virtual page on disk             | 


**** Practice Problem 9.2

determine the number of PTEs that are needed for the following
combinations of virtual address size (n) and page size (P):

#+BEGIN_SRC 
nPTEs = (1<<n) / P
#+END_SRC

|  n | $P = 2^p$ | Number of PTEs |
|----+-----------+----------------|
| 12 | 1K        |              4 |
| 16 | 16K       |              4 |
| 24 | 2M        |              8 |
| 36 | 1G        |             64 |

*** Page Hits

When visiting the cached virtual page, the MMU will translate the
address to the real address in physical memory.

*** Page Faults

/page fault/: a virtual page cache miss happened when MMU is trying to get the physical memory.

A /page fault exception/ will invoke the handler in kernel to select a victim page and
+ copy to disk if another virtual page (VPx) mapped to this PP (PPx) and has been modified
+ disassociate the PTE for VPx (uncached)
+ copy the disk cache to PPx in memory
+ update PTE this VP
+ return from handler
+ restarts the faulting instruction

The virtual memory block is also known as /pages/. The activity of
transferring a page between disk and memory is known as /swapping/ or
/paging/. Pages are /swapped in (paged in)/ from disk to DRAM, and
/swapped out (paged out)/ from DRAM to disk. The strategy of waiting
until the last moment to swap in a page, when a miss occurs, is known
as /demand paging/.

*** Allocating Page

After previous PP being swapped out, its VPE now points to a block located on disk.

*** Locality to the Rescue Again

Virtual memory's performance is benefited by the /locality/. The
principle of locality promises that at any point in time they will
tend to work on a smaller set of /active pages/ known as the /working
set/ or /resident set/.

Terms /thrashing/ means pages are swapped in and out continuously.

In Linux, we can use =getrusage= (get resource usgae) API to check the
page faults ratio of a process (or process group). See
=rusage.ru_majflt= field in =man 2 getrusage=.

** DONE VM as a Tool for Memory Management

By using seperated page tables, OS can provide sepearte virtual address space for each process.
Notice that multiple virtual pages can be mapped to the same shared physical page.
#+CAPTION: VM for memory management
[[./img/multiple-page-table.png]]

The combination of demand paging and separate virtual address spaces
has a profound impact on the way that memory is used and managed in a
system:
- simplifying linking :: each process can use the same basic format
     (layout of segments) for its memory image
- simplifying loading :: allocate virtual pages for code and data
     segments, and marks them as uncached, and points their PET to the
     location object files (load segment as needed). See also =mmap=
- simplifying sharing :: map private virtual pages to disjoint
     physical pages. And share common library, such as =printf=,
     between different processes (share the same physical pages)
- simplifying memory allocation :: the virtual memory allocated by
     =malloc= can scatter randomly in physical memory

** DONE VM as a Tool for Memory Protection

MMU provides access control by read the PTE's permission bits. Any
violation will trigger a general *protection fault* and get a
=SIGSEGV= signal in the offending process (the notorious *segmentation
fault* error)

| Bit Name | Description                                                   |
|----------+---------------------------------------------------------------|
| SUP      | Supervisor: must be running in kernel mode to access the page |
| READ     | read permission to the page                                   |
| WRITE    | write permission to the page                                  |

** DONE Address Translation
*** Overview

The page size (P bytes) is same in virtual space and physical space.
Virtual space may be smaller, or equal, or larger than physical
address space.

Address translation is a mapping between the element of an N-element
virtual address space (VAS) and an M-element physical address space (PAS)
$$ MAP: {VAS} \rightarrow  {PAS} \cup \emptyset $$

A /page fault/ exception will be triggered if the data at virtual addr
is not present in physical memory.

**** Terms of virtual address

| symbol | description                 |
|--------+-----------------------------|
| VPO    | virtual page offset (bytes) |
| VPN    | virtual page number         |
| TLBI   | TLB index                   |
| TLBT   | TLB tag                     |
| TLB    | table base register         |

**** Terms of physical address

| symbol | description                    |
|--------+--------------------------------|
| PPO    | physical page offset (bytes)   |
| PPN    | physical page number           |
| CO     | byte offset within cache block |
| CI     | cache index                    |
| CT     | cache tag                      |

**** Address Translation

/page table base register/ (PTBR) in CPU points to current page table.

An virtual address is constructed with two parts:
1. /p/-bit /virtual page offset/ (VPO)
2. /(n-p)/-bit /virtual page number/ (VPN)

MMU use the VPN to select the appropriate PTE (pointed by PTBR) and
get the /physical page number/ (PPN). Then calculate the real physical
address by add the offset (VPO/PPO) to the physical base address.

#+CAPTION: Address translation with a page table
[[./img/address-translation.png]]


**** page hit

The page hit steps are
1. CPU generate a VA and send it to the MMU
2. MMU generate the PTE address and requests it from the cache/main memory
3. The cache/main memory returns the PTE to the MMU
4. The MMU constructs the physical address and sends it to the cache/main memory
5. The cache/main memory returns the requested data word to the processor.

#+CAPTION: Page hit
[[./img/page-hit.png]]

**** page fault

The page fault is handled by operating system kernel
1. CPU generate a VA and send it to the MMU
2. MMU generate the PTE address and requests it from the cache/main memory
3. MMU triggers an exception if the valid bit in the PTE is zero and
   transfer control in the CPU to a page fault exception handler
4. The fault handler identifies a victim page in physical memory, and
   if that page has been modified, pages it out to disk.
5. The fault handler pages in the new page and update the PTE in memory.
6. The fault handler returns to the origin process, causing the
   faulting instruction to be restarted.
7. back to the page hit steps.

#+CAPTION: Page fault
[[./img/page-fault.png]]

**** Practice Problem 9.3

Given a 64-bit virtual address space and a 32-bit physical address,
determine the number of bits in the VPN, VPO, PPN and PPO for each
page size P:

| P    | VPN | VPO | PPN | PPO |
|------+-----+-----+-----+-----|
| 1KB  |  54 |  10 |  22 |  10 |
| 2KB  |  53 |  11 |  21 |  11 |
| 4KB  |  52 |  12 |  20 |  12 |
| 16KB |  50 |  14 |  18 |  14 |

*** Integrating Caches and VM

SRAM caches DRAM which can be access via either virtual or physical
addresses.

Most systems opt for physical addressing
- straightforward for multiple processes to have blocks in the cache at the same time
- share blocks from the same virtual pages
- doesn't have to deal with protection issues

The main idea is that the address translation occurs before the cache lookup

#+CAPTION: Integrating VM with a physically addressed cache
[[./img/vm-with-pa-cache.png]]

*** Speeding Up Address Translation with a TLB

PTE table is also placed in memory. In the worst case, MMU requires an
additional fetch from memory, at a cost of tens to hundreds of cycles.

Many systems include a small cache of PTEs in the MMU called a
/translation lookaside buffer/ (TLB).

TLB is a small, virtually addressed cache where each line holds a
block consisting of a single PTE.

#+CAPTION: Components of a VA to access the TLB
[[./img/va-tlb.png]]

VPN is consist by /TLB index/ and /TLB tag/
- /TLB index/ consists of the /t/ least significant bits of the VPN 'cause TLB has $T=2^t$ sets
- /TLB tag/ consists of the remaining bits in the VPN.

#+CAPTION: Address Translation with a TLB
[[./img/tlb-hit-and-miss.png]]

The steps of speeding up address access with TLB are
1. CPU generate VA
2. MMU fetch the appropriate PTE from the TLB (or fetches from L1 cache if TLB is missing and caches it to TLB)
3. MMU translate the VA to PA and send it to the cache/main memory
4. The cache/main memory returns the requested data word to the CPU


*** Multi-Level Page Tables

For 32-bit address space, 4KB page and a 4-byte PTE
- there will be $(1 << 32) / (1 << 12)= 1M$ pages
- we need a 4 MB page table resident in memory at all times

The common approach for compacting the page table is to use a
*hierarchy of page tables* instead. For example, we can build 
a two-level page table hierarchy for VA space

#+CAPTION: Two-level page table hierarchy
[[./img/two-level-page-table-hierarchy.png]]

split the 4 MB PTE into two level
- level 1 (contains 1K PTE) :: map to a page which contain 1K PTE
- level 2 (contains 1K PTE) :: map to a page of virtual memory

If the chunk /i/ of level 2 is all unallocated, then level 1 PTE /i/
is null.

This scheme reduces memory requirements in two ways
1. if a PTE in level 1 table is null, then the corresponding level 2 page table does not even have to exist
2. only level 1 table (4 KB) need to be in main memory at all times.

Figure below summarizes address translation with a /k-level/ page table hierarchy:
#+CAPTION: Address translation with a k-level page table
[[./img/address-translation-with-k-level-page-table.png]]
- Virtual address is partitioned into /k/ VPNs and a VPO
- each VPN i is an index into a page table at level /i/
- each PTE in a level /j/ table points to the base of some page table at level $j + 1$
- each PTE in a level /k/ (at last VPN) table contains either the PPN of some physical page or the address of a disk block
- MMU must access /k/ PTEs before it can determine the PPN
- PPO is identical to the VPO

Accessing /k/ PTEs may seem expensive and impractical at first glance. However,
the TLB comes to the rescue here by caching PTEs from the page tables at the 
different levels. In practice, address translation with multi-level page tables is 
not significantly slower than with single-level page tables.

*** Putting It Together: End-to-End Address Translation

Concrete example of end-to-end address translation with a TLB and L1 d-cache.

| Item             | Specification                                                  |
|------------------+----------------------------------------------------------------|
| virtual address  | n = 14                                                         |
| physical address | m = 12                                                         |
| page size        | P = 64                                                         |
| TLB              | 4-way set (2-bit TLBI) associative with 16 total entries[fn:3] |
| L1 d-cache       | 4-byte line size and 16 total sets                             |

Design of VPN/PPN/VPO/PPO:
#+CAPTION: Addressing for small memory system
[[./img/addressing-for-small-memory-system.png]]
- the low-order 6 bits of the VA and PA serve as the VPO and PPO
- the high-order 8 bits of the virtual address serve as the VPN
- the high-order 6 bits of the physical address serve as the PPN

Design of the TLB
- the high-order 6 bits of the VPN serve as TLBT
- the low-order 2 bits of the VPN serve as TLBI
- TLB can cache 4 x 4 entries at most

Desgin of the page table
- 256 page table entries
- these VPNs are not part of the page table and not stored in memory

Desgin of the Cache
- low-order 2 bits serve as offset (CO)
- next 4 bits serve as the set index (CI)
- remaining 6 bits serve as the tag (CT)

If the resulting PTE is invalid, then there is a page fault and the
kernel must page in the appropriate page and return the load
instruction.

**** Notes

14bit 的虚拟地址空间，寻址范围为 16KB；12 bit 物理地址空间，寻址范围为
4KB。页大小为 64B ($2^6$），所以 VPO 和 PPO 均为 6 字节。VPN 为 8 字节，
对应于 256 个PTE。

当不使用 TLB 和 L1-D 缓存时，MMU 在 PTE 中寻找第 i 个表顶，
找出对应的 PPN 地址。根据 $(PPN << 6) | PPO$ 得到最终的物理地址。

TLB 中缓存的 16 个 TLE 条目。其中每种 TLBI 缓存四条，实际每种 TLBI 中有 64 条 TLBT。
MMU 首先查询是否在 TLB 中有对应在 (TLBI, TLBT) 缓存，如果存在，则直接得到 PPN 地址，
否则需要在 PTE 表中进行查询，并将结构写入 TLB 缓存中。

物理地址长度为 12, L1-D 缓存中最多可以保存 16 个 4 字节的物理地址数据。
取物理地址的最低两位为偏移量(CO)，次四位为索引（CI）指向 L1-D 的十六个元素，
高 6 位内容作为比较内容。

#+BEGIN_SRC go
  type TLE struct {
     Tag   bit6
     Valid bool
     PPN   bit6
  }

  type L1CacheEntry struct {
     Tag   bit6
     Valid bool
     Bytes [4]byte
  }

  var L1Cache [16]L1CacheEntry

  // TLB caches 16 of TLEs which can be looked up by the TLBI and TLBT
  var TLB [4]map[bit6]TLE

  func AddressTranslateWithTLB(va bit14) (pa bit12) {
    VPO := va & 0x3F
    VPN := va >> 6
    TLBI := VPN & 0x11            // [0,4)
    TLBT := VPN >> 2              // [0,32)
    // tlb hit
    if tle, ok := TLB[TLBI][TLBT]; ok {
      return (tle.PPN << 6) | VPO
    } else {
      pa := AddressTranslate(va)
      CacheInTLB(pa, va)
      return pa
    }
  }

  func ReadPAByteWithCache(pa bit12) byte{
    CO := (pa & 0x11)             // 2bit
    CI := ((pa >> 2) & 0x0F)      // 4bit
    CT := (pa >> 6)               // 6bit
    entry := L1Cache[CI]
    // cache hit
    if entry.Valid && entry.Tag == CT {
      return entry.Bytes[CO]
    } else {
      byte := ReadPAByte(pa)
      CacheInL1D(byte, pa)
      return byte
    }
  }
#+END_SRC


**** Practice Problem 9.4

Address translation
| Parameter   | Value |
|-------------+-------|
| VPN         |  0x0F |
| VPO/PPO     |  0x27 |
| TLB index   |     3 |
| TLB tag     |     3 |
| TLB hit?    |   Yes |
| PPN         |  0x0D |
| Page fault? |    No |

Physical address is =0x357=

Physical memory reference
| Parameter           | Value |
|---------------------+-------|
| Byte offset         |     3 |
| Cache index         |     5 |
| Cache Tag           |  0x0D |
| Cache hit?          |   Yes |
| Cache byte returned |  0x1D |

** DONE Case Study: The Intel Core i7/Linux Memory System

#+CAPTION: The Core i7 memory system
[[./img/core-i7-memory-system.png]]

/processor package/ (chip) includes 
- four cores
- L3 cache shared by all of the cores
- DDR3 memory controller

Each core contains 
- a hierarchy of TLBs
- a hierarchy of data and instruction caches
- a set of fast point-to-point links, based on the QuickPath technology, to communicate directly with other cores

The TLBs are virtually addressed, and 4-way set associative.
The L1, L2, and L3 caches are physically addressed, with a block
size of 64 bytes.

*** TODO physical addressed vs virtual addressed
*** TODO N-way set associative

Associate Array? Fix sized hash table?

*** Core i7 Address Translation

#+CAPTION: Summary of Core i7 address translation
[[./img/core-i7-address-translation-summary.png]]

Use a four-level page table hierarchy and each process has its own
private page table hierarchy.

The /CR3/ control register contains the physical address of the
beginning of the level 1 (L1) page table. The value of CR3 is part of
each process context, and is stored during each context switch.

#+CAPTION: Format of level 1, level 2, and level 3 page table entries
[[./img/format-of-level-1-to-3-page-table-entries.png]]

For level 1 to 3 page table, when $P=1$ (linux case), the address
field contains a 40-bit physical page number (PPN) that points to the
beginning of the appropriate page table.

#+CAPTION: Format of level 4 page table entries
[[./img/format-of-level-4-page-table-entries.png]]

For level 4 page table, when $P=1$, the address field contains a
40-bit PPN that points to the base of some page in physical memory.


PTE has 3 permission bits that control access to the page
- R/W bit :: read/write or read only
- U/S bit :: user mode, or kernel mode
- XD bit :: disable instruction fetches from individual memory pages

As the MMU translates each virtual address, it also updates two other
bits that can be used by the kernel's page fault handler
- A bit :: reference bit, set for each time a page is accessed, help to implement its page replacement algorithm
- D bit :: dirty bit, set when page is written to, tell kernel whether to write back a victim page before it copies in a replacement page

#+CAPTION: Core i7 page table translation
[[./img/core-i7-page-table-translation.png]]

Core i7 MMU uses the four levels of page tables to translate a virtual
address to a physical address.
- 36 bit VPN is split into four 9-bit chunks
- CR3 register contains the physical address of the L1 page table
- ${PTE}_i$ contains the base address of the $L_{i+1}$ page table

*** Linux Virtual Memory System

Linux's virtual memory are organized as a collection of /areas/ (/segments/)
- code segment
- data segment
- heap
- shared library segment
- user stack

**** Linux Virtual Memory Areas

#+CAPTION: How Linux organizes virtual memory
[[./img/how-linux-organizes-virtual-memory.png]]

Each process associates with =task_struct= object which has a field
named =mm_struct= that characterizes the current state of the virtual
memory
- =pgd= :: points to the base of the level 1 table (the page global directory), CR3 control register
- =mmap= :: points to a list of =vm_area_structs= (area structs)

Each segment is represented by =vm_area_struct=
| field      | description                                               |
|------------+-----------------------------------------------------------|
| =vm_start= | begin of the area                                         |
| =vm_end=   | end of the area                                           |
| =vm_prot=  | r/w permission for all of the pages contained in the area |
| =vm_flags= | permission, accessable, shareable flags                   |
| =vm_next=  | points to the next area struct in the list                |

**** Linux Page Fault Exception Handling

#+CAPTION: Linux page fault handling
[[./img/linux-page-fault-handling.png]]

MMU triggers a page fault while trying to translate some virtual
address A.  The exception results in a tranfer of control to the
kernel's page fault handler, which then performs the following steps:
- Is VA =A= legal? :: search the list of area structs, or trigger a
  segmentation fault and terminates the process
- Is the access legal? :: does the process have permission to read,
  write, or execute the pages in this area? or trigger a protection
  exception, which terminates the process
- handle the page fault :: select a victim page, swapping out the
  victim page if it is dirty, swapping in the new page, and updating
  the page table. And restart the faulting instruction after the page
  fault handler return

** TODO Memory Mapping

Linux maps an /object/ on disk to a virtual memory. (=mmap=)

There're two types of objects
1. Regular file in the Linux file system: map to file section, divided into page-size pieces
2. Anonymous file: *no data are actually transferred between disk and memory* (demand-zero pages)

Pages are swapped back and forth between a special swap file maintained by the kernel.

*** Shared Objects Revisited

Avoid to load shared program (=bash=) or libraries (=libc=) into each
process' own memory. But using memory mapping for controlling how
objects are shared by multiple processes.

**** shared object

[[./img/mmap-shared-object.png]]

Kernel can quickly determine the process 1 has already mapped this object and can
point the page table entries in process 2 to the appropriate physical pages.

Only a single copy of the shared object needs to be stored in physical
memory, even though the object is mapped into multiple shared areas.


**** private object

#+CAPTION: =copy-on-write= technique

[[./img/mmap-private-object.png]]

1. the page table entries for the corresponding private area are
   flagged as read-only, and the are struct is flagged as /private
   copy-on-write/.
2. write to some page in the private area triggers a *protection
   fault*
3. fault handler create a new copy of the page in physical memory,
   updates the page table entry to point to the new copy, and then
   restores write permissions to the page
4. CPU re-executes the write

*** The =fork= Function Revisited

To fork a new process, it creates exact copie of current process's
=mm_struct=, area structs, and page tables.  It flags each page in
both processes as read-only, and flags each area struct in both
processes as private copy-on-write.

*** The =execve= Function Revisited



#+BEGIN_QUOTE
execve() does not return on success, and the text, initialized data,
uninitialized data (bss), and stack of the calling process are
overwritten according to the contents of the newly loaded program.
#+END_QUOTE

[[file:img/execve.png]]

Loading and running =a.out= requires the following steps:

1. Delete existing user ares
2. map private areas: Create new area structs for the code, data, bss,
   and stack ares of the new program (private copy-on-write)
   1. The code and data areas are mapped to the =.text= and =.data= sections
   2. The bss area is demand-zero, mapped to an anonymous file whose
      size is contained in =a.out=
   3. The stack and heap are are also demand-zero, initially of zero length
3. Map shared areas (dynamic link shared libraries)
4. Set the program counter (PC) to the entry point in the code area

*** User-Level Memory Mapping with the =mmap= Function

Using =mmap= function to create new areas of virtual memory and to map
objects into these areas

#+BEGIN_SRC c
  /* 
   ,* map file descriptor `fd`'s content from offset with length to area `starts` address (preferably)
   ,* the `start` address is merely a hint, the real start address will chose by the kernel
   ,*
   ,* `prot` contains bits that describe the access permissions:
   ,* 1. PROT_EXEC
   ,* 2. PROT_READ
   ,* 3. PROT_WRITE
   ,* 4. PROT_NONE
   ,*
   ,* flags consists of bits that describe the type of the mapped object
   ,* 1. MAP_ANON: backing store is an anonymous object and the corresponding virtual pages are demand-zero
   ,* 2. MAP_PRIVATE: private copy-on-write object
   ,* 3. MAP_SHARED: shared object
   ,*/
  void *mmap(void *start, size_t length, int prot, int flags, int fd, off_t offset);
#+END_SRC

**** Solution of Practice Problem 9.5
#+BEGIN_SRC c
  #include <fcntl.h>
  #include <sys/mman.h>
  #include <sys/stat.h>
  #include <sys/types.h>
  #include <unistd.h>

  #include <errno.h>
  #include <stdio.h>
  #include <stdlib.h>
  #include <string.h>

  #define BLOCK_SIZE 1024

  int main(int argc, char *argv[]) {
    if (argc != 2) {
      fprintf(stderr, "Usage: %s <input>\n", argv[0]);
      exit(0);
    }

    const char *fpath = argv[1];
    int fd = open(fpath, O_RDONLY);
    if (fd == -1) {
      fprintf(stderr, "open file %s failed: %s\n", fpath, strerror(errno));
      exit(-1);
    }

    struct stat stat = {0};
    if (fstat(fd, &stat) == -1) {
      fprintf(stderr, "get file stat %s failed: %s\n", fpath, strerror(errno));
      exit(-1);
    }

    void *src = mmap(NULL, stat.st_size, PROT_READ, MAP_SHARED, fd, 0);
    if (src == NULL) {
      fprintf(stderr, "mmap %s failed: %s\n", fpath, strerror(errno));
      exit(-1);
    }
    char *ptr = (char *)src;
    char *end = ptr + stat.st_size;
    while (ptr < end) {
      int s = end - ptr;
      if (s > BLOCK_SIZE) {
        s = BLOCK_SIZE;
      }
      write(STDOUT_FILENO, ptr, s);
      ptr += s;
    }
    return 0;
  }

#+END_SRC

*** TODO Question

- When we install a Linux and don't mount a swap partition, where the virtual page is swapped out to?
- =set ULIMIT=0= dump process memory?

** TODO Dynamic Memory Allocation

** TODO Garbage Collection

** TODO Common Memory-Related Bugs in C Programs

** TODO Summary

* Footnotes

[fn:1] Controller 集成在设备或主板上，Adapater 是主板上的插槽的可插拔设备
[fn:2] 指令集( Instruction set artichecture ) 相当于对外的接口，CPU 具体实现可以不同，称为 microarchitecture
[fn:3] 为什么不像 D1-Cache 直接作成 16 个元素的一维表？


