#+TITLE: Kubernetes 笔记
#+AUTHOR: Zhao Wenbin

* Overview

Kubernetes Control Plane (aka Kubernetes) consists of
1. Master
2. Node

And the major works of kubernetes are:
1. execute instructions (interact)
2. making the cluster's actual state match the desired state (maintain)

We interact with Kubernetes cluster and check the status of Kubernetes
with API (JSON objects)

Kubernetes would maintain the status of cluster and jobs, such like
1. start or restart containers
2. scaling the number of replicas of an Application

** Master

Consists of three process:
1. =kube-apiserver=
2. =kube-controller-manager=
3. =kube-scheduler=

** Node

Node components run on every node
1. maintain running Pods
2. provide the Kubernetes runtime environment

On each node, there're several components running:
1. kubelet: Run container as the rules of =PodSpecs=
2. kube-proxy: maintain network configuration for supporting Service
3. container runtime: docker, rkt, OCI runtime-spec

** Feature

From [[https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/courseware/8835181d87b046b697603bd83f242f16/669a580b34764a0cadee817202b8c74a/?child=first][Kubernetes Features I]] and [[https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/courseware/8835181d87b046b697603bd83f242f16/669a580b34764a0cadee817202b8c74a/?child=first][Kubernetes Feature II]]

- Automatic binpacking :: Kubernetes automatically schedules the containers based on resource usage and constraints, without sacrificing the availability
- Self-healing :: Kubernetes automatically replaces and reschedules the containers from failed nodes. It also kills and restarts the containers which do not respond to health checks, based on existing rules/policy.
- Horizontal scaling :: Kubernetes can automatically scale applications based on resource usage like CPU and memory. In some cases, it also supports dynamic scaling based on customer metrics.
- Service discovery and Load balancing :: Kubernetes groups sets of containers and referes to them via a Domain Name System (DNS). This DNS is also called a Kubernetes *service*. Kubernetes can discover these services automatically, and load-balance requests between containers of a given service.
- Automated rollouts and rollbacks :: Kubernetes can roll out and roll back new version/configurations of an application, without introducing any downtime.
- Secrets and configuration management :: Kubernetes can manages secrets and configuration details for an application without re-building the respective images. With secrets, we can share confidential information to our application without exposing it to the stack configuration, like on Github.
- Storage orchestration :: With Kubernetes and its plugins, we can automatically mount local, external, and storage solutions to the containers in a seamless manner, based on software-defined storage (SDS).
- Batch execution :: Besides long running jobs, Kubernetes also supports batch execution.

** Objects

Basic objects in Kubernetes are
- pod :: a group of running instances
- service :: policy to expose network
- volume :: persist storage
- namespace :: isolate resources

In high level, there're some abstracts named *controller*, such as
- ReplicaSet
- Deployment
- StatefulSet
- DaemonSet
- Job

* Pods

** What is?

+ Smallest deployable units of computing that can be created and managed in k8s cluster
+ A group of Docker containers with shared namespaces and shared volumes
  - a group of containers, relatively tightly coupled
  - shared storage/network
  - with specification for how to run the containers
  - containers share an *IP address and port space*, can communicate with each-other via *localhost*
  - volume provides shared filesystem and data persistent
+ Some notes about underlying
  + supports more container runtimes (beyond Docker)
  + undergroupd is a set of Linux namespaces, cgroups, and potentially other facets of isolation

** TODO life of a pod

- relatively ephemeral
- volume has the same lifetime as the pod
- https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/

** TODO replication

pod 的一个 node 异常后，将重新发起一个新的 pod 替换之

https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/



** Motivation

- Serve as unit of deployment, horizontal scaling, and replication.
- Automatically handles:
  - Colocation (co-scheduling)
  - shared fate (termination together)
  - coordinated replication
  - resource sharing
  - dependency management

** TODO Usage

** TODO Terminates 


*** TODO How to find each other by *localhost*

* Volume

** vs Docker Volume

docker volume 是将本地的一个目录挂载到 container 中，存在以下问题
1. 实现方式单一（docker driver 提供了一定的扩展功能）
2. 不能在多个容器之间共享
3. 没有生命周期管理（独立于 container）

k8s volume 有以下特点：
1. 与 Pod 生命周期相同
2. 当 Pod 容器重启后，volume 依旧存在
3. Pod 中的窗口共享 volume
4. 有很多种 backend 实现方式

** PersistentVolume & PersistentVolumeClaims

*** PV vs PVC

- PVs are resources in the cluster
- PVCs are requests for those resources and also act as /claim checks/ to the resource.

*** Lifecycle

**** Provisioning

1. Static: Administration allocated
2. Dynamic: automatic provision for PVC request, based on =StorageClasses=

**** Binding

- PVC request amount of storage and access mode
- Master find a matching PV for PVC and binding them together

**** Using

* Deployments
* ReplicaSets

- Use =Deployment= instead when possible to support rolling update feature
- =Deployment= is a high-level abstract of =ReplicaSets=
- Similar to =ReplicationController=, but support set-based selector
- Specific pod configuration by =spec.template=

* Network

Pods had private cluster-IP which means:
1. containers in Pod can reach each other's port on =localhost=
2. All pods can see each other without NAT

** vs Docker Bridge Network

Docker's conatiners can visit others on the same machine. 

But for communicating across machine, user must setup proxy to forward
traffic to specific ports.

* Service

** Overview

Pod has internal and temporary cluster-IP address which is
easy-to-dead.

Kubernetes provides Deployment or ReplicaSet controller for monitoring
pods status which would spawn new pods with different cluster-IP
address when necessary.

=Service= is an abstraction which defines a logical set of =Pods= and
a policy by which to access them. Use =service= to decouple frontend
pod with backend pod connection.

Service is assigned a unique IP address which is independent with
pods. Traffic will be routed to some pod in the groups of service.

** How to create services

1. =kubectl expose <deployment>=
2. create by =Service= API
   #+BEGIN_SRC yaml
      kind: Service
     apiVersion: v1
     metadata:
       name: my-service
     spec:
       selector:
         app: MyApp
       ports:
       - protocol: TCP
         port: 80
         targetPort: 9376
   #+END_SRC

** =Endpoints=

=Endpoints= is an object for Pods and will update whenever set of
=Pods= in a =Service= changed.

For non-native application, use virtual-IP-based bridge to Service.

When create =Service=, an =Endpoints= will be created with the same
name as Service (must have label selectors)


** Label Selectors

- Worked just as a route item, map port to target port which is not part of =Pods=
- Because no selector specific, there will not be =Endpoints= be created automatically
- Create =Endpoint= manual
- [[*ExternalName service][ExternalName service]] is a special case, see below

** Network

Pod has cluster-IP and can be visited in other cluster's nodes or pods.




** Define a service

- Would be assigned an IP address (cluster-IP), which is used by proxy
- An =Endpoints= also named 'my-service' will be created automatically
- Map an incompoint =port= to any =targetPort=
** Proxy

- Each node has a process named =kube-proxy=
- Which is responsible for implementing a form of virtual IP for =Services=
- There are three mode for =Proxy=
  + Userspace
    1. iptable rules to forward =ClusterIP:Port= to =localhost:ProxyPort=
    2. proxied =localhost:ProxyPort='s traffic to one of the =Service='s backend =Pods=
  + Iptables
    - Set iptable rule for each =Endpoints= 
    - Random forward request to =CLusterIP:Port= to backend =Pod=
    - But can not retry for failed, depends on having working readiness probes
  + ipvs (new in v1.9, skip)

** Find =Service=

=Service= clusterIP is choosed randomly, Two methods to find =Service= clusterIP

*** Environment

- After create =Services=, new =Pods= will be configured with environment variables
  #+BEGIN_SRC bash
  ${SVCNAME}_SERVICE_HOST
  ${SVCNAME}_SERVICE_PORT
  #+END_SRC
- To use env, Services must be created before Pods

*** DNS

- =kube-system/kube-dns= pod provide DNS service
- DNS Server monitor the changes of =Services= and maintain items to service IP
- Map =service-name.namespace= to Service cluster IP

** Publish services to external

Some kind of service by setting =Type= value (ServiceType)
1. ClusterIP: for cluster-internal usage
2. NodePort: Can be visited from external by =<NodeIP>:<NodePort>= (every node)
3. LoadBalancer: Expose the service externally using a cloud provider's load balancer(?)
4. ExternalName: return a =CNAME= record with =externalName= field's value (?)

** TODO Shortcomings

** Misc

- =Services= can expose more than one port and you must give all of your ports names. For examples:
  #+BEGIN_SRC yaml
    kind: Service
    apiVersion: v1
    metadata:
      name: my-service
    spec:
      selector:
        app: MyApp
      ports:
      - name: http
        protocol: TCP
        port: 80
        targetPort: 9376
      - name: https
        protocol: TCP
        port: 443
        targetPort: 9377
  #+END_SRC
- Can set clusterIP of =Services= by =.spec.clusterIP=. Most useful for legacy system which configured a specific IP address
* TODO ConfigMap

Something like register or key/value database(?)

Can be mounted to container's file.
* TODO TODO
** Prove containers' ports may conflict under docker bridge networks
** link: how we archieve this
** Pod spec containerPort
** Pod's container port and targetPort
** =printenv= vs =env=
