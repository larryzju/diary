#+TITLE: Kubernetes 笔记
#+AUTHOR: Zhao Wenbin

* Overview
** Components
*** Node
**** Overview

Node components run on every node
1. maintain running Pods
2. provide the Kubernetes runtime environment

**** Components

1. kubelet: Run ocntainer as the rules of =PodSpecs=
2. kube-proxy: maintain network configuration for supporting Service
3. container runtime: docker, rkt, OCI runtime-spec

** Feature

From [[https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/courseware/8835181d87b046b697603bd83f242f16/669a580b34764a0cadee817202b8c74a/?child=first][Kubernetes Features I]] and [[https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS158x+1T2018/courseware/8835181d87b046b697603bd83f242f16/669a580b34764a0cadee817202b8c74a/?child=first][Kubernetes Feature II]]

- Automatic binpacking :: Kubernetes automatically schedules the containers based on resource usage and constraints, without sacrificing the availability
- Self-healing :: Kubernetes automatically replaces and reschedules the containers from failed nodes. It also kills and restarts the containers which do not respond to health checks, based on existing rules/policy.
- Horizontal scaling :: Kubernetes can automatically scale applicatoins based on resource usage like CPU and memory. In some cases, it also supports dynamic scaling based on customer metrics.
- Service discovery and Load balancing :: Kubernetes groups sets of containers and referes to them via a Domain Name System (DNS). This DNS is also called a Kubernetes *service*. Kubernetes can discover these services automatically, and load-balance requests between containers of a given service.
- Automated rollouts and rollbacks :: Kubernetes can roll out and roll back ne wversion/configurations of an application, without introducing any downtime.
- Secrets and configuration management :: Kubernetes can manages secrets and configuration details for an applicatoin without re-building the respective images. With secrets, we can share confidential information to our application without exposing it to the stack configuration, like on Github.
- Storage orchestration :: With Kubernetes and its plugins, we can automatically mount local, external, and storage solutions to the containers in a seamless manner, based on software-defined storage (SDS).
- Batch execution :: Besides long running jobs, Kubernetes also supports batch execution.


* Pods

** definition

- Smallest deployable units of computing that can be created and managed in K8S
- A group of Docker containers with shared namespaces and shared volumes


** explain

- a group of containers, relatively tightly coupled
- shared storage/network
- with specification for how to run the containers
- TODO containers share an *IP address and port space*, can communicate with each-other via *localhost*
- volume provides shared filesystem

** notes

- supports more container runtimes (beyond Docker)
- undergroupd is a set of Linux namespaces, cgroups, and potentially other facets of isolation

** TODO life of a pod

- relatively ephemeral
- volume has the same lifetime as the pod
- https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/

** TODO replication

pod 的一个 node 异常后，将重新发起一个新的 pod 替换之

https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/

** Motivation

*** Management

Serve as unit of deployment, horizontal scaling, and replicatoin.

Automatically handles:

- Colocation (co-scheduling)
- shared fate (termination)
- coordinated replication
- resource sharing
- dependency management

** Resource sharing and communication

- can find each other and communicate using *localhost*
- volume for
  + persist data for container restart
  + share among applications

** TODO Usage

** TODO Terminates 


*** TODO How to find each other by *localhost*

* Networking

* Volume

** 不同于 docker volume

docker volume 是将本地的一个目录挂载到 container 中，存在以下问题
1. 实现方式单一（docker driver 提供了一定的扩展功能）
2. 不能在多个容器之间共享
3. 没有生命周期管理（独立于 container）

k8s volume 有以下特点：
1. 与 Pod 生命周期相同
2. 当 Pod 容器重启后，volume 依旧存在
3. Pod 中的窗口共享 volume
4. 有很多种 backend 实现方式

** PersistentVolume & PersistentVolumeClaims

*** PV vs PVC

- PVs are resources in the cluster
- PVCs are requests for those resources and also act as /claim checks/ to the resource.

*** Lifecycle

**** Provisioning

1. Static: Administration allocated
2. Dynamic: automatic provision for PVC request, based on =StorageClasses=

**** Binding

- PVC request amount of storage and access mode
- Master find a matching PV for PVC and binding them together

**** Using


* Deployments
* ReplicaSets

- Use =Deployment= instead when possible to support rolling update feature
- =Deployment= is a high-level abstract of =ReplicaSets=
- Similar to =ReplicationController=, but support set-based selector
- Specific pod configuration by =spec.template=

* Service

** Why service

- For pod has internal and temporary IP address.
- Use =service= to decouple frontend pod with backend pod connection

** As micro-service

=Service= is an abstraction which defines a logical set of =Pods= and a policy by which to access them.

** =Endpoints=

- For k8s-native applications
- =Endpoints= is updated whenever then set of =Pods= in a =Service= changed
- For non-native application, use virtual-IP-based bridge to Service
- When create =Service=, an =Endpoints= will be created with the same name as Service (must have label selectors)

** Services without label selectors

- Worked just as a route item, map port to target port which is not part of =Pods=
- Because no selector specific, there will not be =Endpoints= be created automatically
- Create =Endpoint= manual
- [[*ExternalName service][ExternalName service]] is a special case, see below

** Define a service

#+BEGIN_SRC yaml
  kind: Service
  apiVersion: v1
  metadata:
    name: my-service
  spec:
    selector:
      app: MyApp
    ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
#+END_SRC

- Would be assigned an IP address (cluster-IP), which is used by proxy
- An =Endpoints= also named 'my-service' will be created automatically
- Map an incompoint =port= to any =targetPort=
** Proxy

- Each node has a process named =kube-proxy=
- Which is responsible for implementing a form of virtual IP for =Services=
- There are three mode for =Proxy=
  + Userspace
    1. iptable rules to forward =ClusterIP:Port= to =localhost:ProxyPort=
    2. proxied =localhost:ProxyPort='s traffic to one of the =Service='s backend =Pods=
  + Iptables
    - Set iptable rule for each =Endpoints= 
    - Random forward request to =CLusterIP:Port= to backend =Pod=
    - But can not retry for failed, depends on having working readiness probes
  + ipvs (new in v1.9, skip)

** Find =Service=

=Service= clusterIP is choosed randomly, Two methods to find =Service= clusterIP

*** Environment

- After create =Services=, new =Pods= will be configured with environment variables
  #+BEGIN_SRC bash
  ${SVCNAME}_SERVICE_HOST
  ${SVCNAME}_SERVICE_PORT
  #+END_SRC
- To use env, Services must be created before Pods

*** DNS

- DNS Server monitor the changes of =Services= and maintain items to service IP
- Map =service-name.namespace= to Service cluster IP

** Publish services to external

Some kind of service by setting =Type= value (ServiceType)
1. ClusterIP: for cluster-internal usage
2. NodePort: Can be visited from external by =<NodeIP>:<NodePort>= (every node)
3. LoadBalancer: Expose the service externally using a cloud provider's load balancer(?)
4. ExternalName: return a =CNAME= record with =externalName= field's value (?)

** TODO Shortcomings

** Misc

- =Services= can expose more than one port and you must give all of your ports names. For examples:
  #+BEGIN_SRC yaml
    kind: Service
    apiVersion: v1
    metadata:
      name: my-service
    spec:
      selector:
        app: MyApp
      ports:
      - name: http
        protocol: TCP
        port: 80
        targetPort: 9376
      - name: https
        protocol: TCP
        port: 443
        targetPort: 9377
  #+END_SRC
- Can set clusterIP of =Services= by =.spec.clusterIP=. Most useful for legacy system which configured a specific IP address
